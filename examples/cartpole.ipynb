{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dnetworks.model import NNet\n",
    "from dnetworks.layers import LinearLayer, LeakyReLU, MSELoss\n",
    "from dnetworks.optimizers import Adam\n",
    "from drlearn import DRLearn\n",
    "from utils import Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(model_rl, n_epochs=10):\n",
    "    for epoch in range(n_epochs):\n",
    "        state = model_rl.env.reset()\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            model_rl.env.render()\n",
    "            state = state.reshape(model_rl.n_states, 1)\n",
    "            action = np.argmax(model_rl.model.forward(state).T, axis=1)[0]\n",
    "            state, reward, done, _ = model_rl.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done: break\n",
    "        print (\"%d Reward: %s\" % (epoch, total_reward))\n",
    "\n",
    "    model_rl.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "\n",
    "# Initialize the environment\n",
    "gym_env = gym.make(env_name)\n",
    "n_states = gym_env.observation_space.shape[0]\n",
    "n_actions = gym_env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NNet()\n",
    "\n",
    "# Create the model structure\n",
    "model.add(LinearLayer(n_states, 64))\n",
    "model.add(LeakyReLU())\n",
    "\n",
    "model.add(LinearLayer(64, 32))\n",
    "model.add(LeakyReLU())\n",
    "\n",
    "model.add(LinearLayer(32, 8))\n",
    "model.add(LeakyReLU())\n",
    "\n",
    "model.add(LinearLayer(8, n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the loss functions and the optimize method\n",
    "loss = MSELoss()\n",
    "optim = Adam(lr=0.001)\n",
    "memory = Memory(capacity=300, sample_size=32)\n",
    "\n",
    "dqn = DRLearn(\n",
    "    model=model,\n",
    "    optim=optim,\n",
    "    loss=loss,\n",
    "    env=gym_env,\n",
    "    memory=memory,\n",
    "    epsilon=0.9, \n",
    "    gamma=0.8, \n",
    "    decay_rate=0.005,\n",
    "    min_epsilon=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "epoch_losses = []\n",
    "max_reward = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss, total_reward = dqn.play()\n",
    "    max_reward = max(max_reward, total_reward)\n",
    "    dqn.update_epsilon(epoch=epoch)\n",
    "    epoch_losses.append(epoch_loss)\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print('loss: ', epoch_loss, 'max reward: ', max_reward, 'epoch: ', epoch)\n",
    "\n",
    "plt.plot(np.convolve(epoch_losses, np.ones(20), 'valid') / 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_game(model_rl=dqn, n_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
